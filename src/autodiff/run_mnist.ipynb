{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f137c1a8eb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 9.80%\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28,1024)\n",
    "        self.fc2 = nn.Linear(1024, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "# import os\n",
    "# if not os.path.exists('mnist_model.pt'):\n",
    "#     #train\n",
    "#     print('training.. ')\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for images, labels in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(images)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "#     torch.save(model,'mnist_model.pt')\n",
    "# else:\n",
    "#     print('loading..')\n",
    "#     model = torch.load('mnist_model.pt')\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = images[3].unsqueeze(0), labels[0].unsqueeze(0)\n",
    "plt.imshow(x.numpy().squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dldw norm: tensor(15.9061)\n"
     ]
    }
   ],
   "source": [
    "out  = model(x)\n",
    "loss = criterion(out,y)\n",
    "dldw_target = torch.autograd.grad(loss, model.fc2.weight)[0]\n",
    "\n",
    "print('dldw norm:',dldw_target.norm())\n",
    "# model = zero_grad(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = torch.nn.Parameter(torch.rand_like(x),requires_grad=True)\n",
    "# x_hat = torch.nn.Parameter(images[1].unsqueeze(0),requires_grad=True)\n",
    "# x_hat_sm = torch.softmax(x_hat)\n",
    "# x_hat = torch.nn.Parameter(x.data,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_hat  = model(x_hat)\n",
    "# loss_hat = criterion(out_hat,y)\n",
    "# dldw_hat = torch.autograd.grad(loss_hat, model.fc2.weight)[0]\n",
    "# print('dldw_hat norm:',dldw_hat.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_loss(x, y, model, dldw_target):\n",
    "    logits = model(x)\n",
    "    loss   = criterion(logits, y)\n",
    "    dldw   = torch.autograd.grad(loss, model.fc2.weight, create_graph=True)[0]\n",
    "    return torch.sum((dldw-dldw_target)**2), dldw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metaloss: tensor(34.0615, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mloss, dldw_f = meta_loss(x=x_hat, y=y, model=model, dldw_target=dldw_target)\n",
    "print('metaloss:', mloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration, Loss, Gradient Norm, Learning Rate:   0, 34.0615, 2.2529, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   1, 33.5558, 2.2365, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   2, 33.0574, 2.2201, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   3, 32.5663, 2.2037, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   4, 32.0825, 2.1872, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   5, 31.6059, 2.1707, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   6, 31.1365, 2.1543, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   7, 30.6742, 2.1378, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   8, 30.2190, 2.1213, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:   9, 29.7707, 2.1048, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  10, 29.3294, 2.0884, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  20, 25.2839, 1.9258, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  30, 21.8567, 1.7692, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  40, 18.9707, 1.6217, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  50, 16.5483, 1.4852, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  60, 14.5162, 1.3606, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  70, 12.8094, 1.2479, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  80, 11.3713, 1.1466, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate:  90, 10.1548, 1.0559, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 100, 9.1206, 0.9749, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 110, 8.2367, 0.9027, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 120, 7.4769, 0.8382, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 130, 6.8201, 0.7805, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 140, 6.2491, 0.7289, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 150, 5.7498, 0.6825, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 160, 5.3110, 0.6408, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 170, 4.9233, 0.6031, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 180, 4.5791, 0.5690, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 190, 4.2720, 0.5381, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 200, 3.9968, 0.5099, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 210, 3.7493, 0.4842, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 220, 3.5256, 0.4606, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 230, 3.3229, 0.4390, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 240, 3.1385, 0.4191, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 250, 2.9701, 0.4008, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 260, 2.8159, 0.3838, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 270, 2.6744, 0.3680, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 280, 2.5441, 0.3534, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 290, 2.4238, 0.3398, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 300, 2.3124, 0.3270, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 310, 2.2092, 0.3151, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 320, 2.1132, 0.3040, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 330, 2.0238, 0.2935, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 340, 1.9404, 0.2837, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 350, 1.8624, 0.2744, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 360, 1.7894, 0.2657, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 370, 1.7209, 0.2574, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 380, 1.6566, 0.2496, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 390, 1.5960, 0.2422, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 400, 1.5389, 0.2352, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 410, 1.4851, 0.2285, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 420, 1.4343, 0.2222, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 430, 1.3862, 0.2162, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 440, 1.3406, 0.2104, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 450, 1.2974, 0.2049, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 460, 1.2565, 0.1997, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 470, 1.2175, 0.1947, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 480, 1.1805, 0.1899, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 490, 1.1453, 0.1853, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 500, 1.1117, 0.1809, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 510, 1.0797, 0.1767, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 520, 1.0491, 0.1727, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 530, 1.0200, 0.1688, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 540, 0.9921, 0.1650, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 550, 0.9654, 0.1614, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 560, 0.9399, 0.1580, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 570, 0.9154, 0.1547, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 580, 0.8920, 0.1514, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 590, 0.8695, 0.1483, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 600, 0.8479, 0.1454, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 610, 0.8272, 0.1425, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 620, 0.8072, 0.1397, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 630, 0.7881, 0.1370, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 640, 0.7697, 0.1344, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 650, 0.7519, 0.1319, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 660, 0.7348, 0.1294, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 670, 0.7184, 0.1271, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 680, 0.7025, 0.1248, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 690, 0.6872, 0.1226, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 700, 0.6724, 0.1204, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 710, 0.6582, 0.1183, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 720, 0.6444, 0.1163, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 730, 0.6311, 0.1144, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 740, 0.6182, 0.1125, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 750, 0.6057, 0.1106, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 760, 0.5937, 0.1088, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 770, 0.5820, 0.1071, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 780, 0.5708, 0.1054, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 790, 0.5598, 0.1037, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 800, 0.5492, 0.1021, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 810, 0.5390, 0.1005, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 820, 0.5290, 0.0990, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 830, 0.5193, 0.0975, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 840, 0.5100, 0.0961, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 850, 0.5008, 0.0947, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 860, 0.4920, 0.0933, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 870, 0.4834, 0.0920, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 880, 0.4751, 0.0907, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 890, 0.4670, 0.0894, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 900, 0.4591, 0.0882, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 910, 0.4514, 0.0870, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 920, 0.4439, 0.0858, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 930, 0.4367, 0.0846, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 940, 0.4296, 0.0835, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 950, 0.4227, 0.0824, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 960, 0.4160, 0.0813, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 970, 0.4095, 0.0803, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 980, 0.4031, 0.0793, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 990, 0.3969, 0.0782, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1000, 0.3909, 0.0773, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1010, 0.3850, 0.0763, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1020, 0.3792, 0.0754, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1030, 0.3736, 0.0745, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1040, 0.3681, 0.0736, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1050, 0.3628, 0.0727, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1060, 0.3575, 0.0718, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1070, 0.3524, 0.0710, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1080, 0.3474, 0.0702, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1090, 0.3426, 0.0694, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1100, 0.3378, 0.0686, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1110, 0.3332, 0.0678, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1120, 0.3286, 0.0670, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1130, 0.3242, 0.0663, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1140, 0.3198, 0.0656, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1150, 0.3156, 0.0649, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1160, 0.3114, 0.0642, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1170, 0.3073, 0.0635, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1180, 0.3033, 0.0628, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1190, 0.2994, 0.0621, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1200, 0.2956, 0.0615, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1210, 0.2919, 0.0609, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1220, 0.2882, 0.0602, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1230, 0.2846, 0.0596, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1240, 0.2811, 0.0590, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1250, 0.2776, 0.0584, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1260, 0.2743, 0.0579, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1270, 0.2709, 0.0573, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1280, 0.2677, 0.0567, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1290, 0.2645, 0.0562, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1300, 0.2614, 0.0556, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1310, 0.2583, 0.0551, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1320, 0.2553, 0.0546, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1330, 0.2523, 0.0541, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1340, 0.2494, 0.0536, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1350, 0.2466, 0.0531, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1360, 0.2438, 0.0526, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1370, 0.2411, 0.0521, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1380, 0.2384, 0.0516, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1390, 0.2357, 0.0512, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1400, 0.2331, 0.0507, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1410, 0.2306, 0.0503, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1420, 0.2281, 0.0498, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1430, 0.2256, 0.0494, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1440, 0.2232, 0.0490, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1450, 0.2208, 0.0486, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1460, 0.2185, 0.0481, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1470, 0.2162, 0.0477, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1480, 0.2139, 0.0473, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1490, 0.2117, 0.0469, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1500, 0.2095, 0.0466, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1510, 0.2074, 0.0462, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1520, 0.2052, 0.0458, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1530, 0.2032, 0.0454, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1540, 0.2011, 0.0451, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1550, 0.1991, 0.0447, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1560, 0.1971, 0.0443, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1570, 0.1952, 0.0440, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1580, 0.1932, 0.0437, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1590, 0.1913, 0.0433, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1600, 0.1895, 0.0430, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1610, 0.1876, 0.0426, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1620, 0.1858, 0.0423, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1630, 0.1841, 0.0420, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1640, 0.1823, 0.0417, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1650, 0.1806, 0.0414, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1660, 0.1789, 0.0411, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1670, 0.1772, 0.0408, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1680, 0.1756, 0.0405, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1690, 0.1739, 0.0402, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1700, 0.1723, 0.0399, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1710, 0.1708, 0.0396, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1720, 0.1692, 0.0393, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1730, 0.1677, 0.0390, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1740, 0.1661, 0.0387, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1750, 0.1647, 0.0385, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1760, 0.1632, 0.0382, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1770, 0.1617, 0.0379, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1780, 0.1603, 0.0377, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1790, 0.1589, 0.0374, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1800, 0.1575, 0.0372, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1810, 0.1561, 0.0369, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1820, 0.1548, 0.0367, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1830, 0.1534, 0.0364, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1840, 0.1521, 0.0362, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1850, 0.1508, 0.0359, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1860, 0.1495, 0.0357, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1870, 0.1483, 0.0355, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1880, 0.1470, 0.0352, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1890, 0.1458, 0.0350, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1900, 0.1446, 0.0348, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1910, 0.1434, 0.0345, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1920, 0.1422, 0.0343, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1930, 0.1410, 0.0341, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1940, 0.1399, 0.0339, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1950, 0.1387, 0.0337, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1960, 0.1376, 0.0335, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1970, 0.1365, 0.0333, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1980, 0.1354, 0.0331, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 1990, 0.1343, 0.0328, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2000, 0.1332, 0.0326, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2010, 0.1322, 0.0324, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2020, 0.1311, 0.0323, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2030, 0.1301, 0.0321, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2040, 0.1291, 0.0319, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2050, 0.1281, 0.0317, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2060, 0.1271, 0.0315, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2070, 0.1261, 0.0313, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2080, 0.1251, 0.0311, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2090, 0.1241, 0.0309, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2100, 0.1232, 0.0307, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2110, 0.1222, 0.0306, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2120, 0.1213, 0.0304, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2130, 0.1204, 0.0302, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2140, 0.1195, 0.0300, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2150, 0.1186, 0.0299, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2160, 0.1177, 0.0297, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2170, 0.1168, 0.0295, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2180, 0.1160, 0.0294, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2190, 0.1151, 0.0292, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2200, 0.1142, 0.0290, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2210, 0.1134, 0.0289, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2220, 0.1126, 0.0287, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2230, 0.1118, 0.0286, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2240, 0.1109, 0.0284, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2250, 0.1101, 0.0283, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2260, 0.1093, 0.0281, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2270, 0.1086, 0.0279, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2280, 0.1078, 0.0278, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2290, 0.1070, 0.0276, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2300, 0.1063, 0.0275, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2310, 0.1055, 0.0274, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2320, 0.1048, 0.0272, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2330, 0.1040, 0.0271, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2340, 0.1033, 0.0269, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2350, 0.1026, 0.0268, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2360, 0.1019, 0.0266, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2370, 0.1012, 0.0265, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2380, 0.1005, 0.0264, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2390, 0.0998, 0.0262, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2400, 0.0991, 0.0261, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2410, 0.0984, 0.0260, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2420, 0.0977, 0.0258, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2430, 0.0971, 0.0257, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2440, 0.0964, 0.0256, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2450, 0.0958, 0.0254, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2460, 0.0951, 0.0253, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2470, 0.0945, 0.0252, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2480, 0.0938, 0.0251, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2490, 0.0932, 0.0249, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2500, 0.0926, 0.0248, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2510, 0.0920, 0.0247, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2520, 0.0914, 0.0246, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2530, 0.0908, 0.0245, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2540, 0.0902, 0.0243, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2550, 0.0896, 0.0242, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2560, 0.0890, 0.0241, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2570, 0.0884, 0.0240, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2580, 0.0878, 0.0239, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2590, 0.0873, 0.0238, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2600, 0.0867, 0.0237, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2610, 0.0862, 0.0235, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2620, 0.0856, 0.0234, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2630, 0.0851, 0.0233, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2640, 0.0845, 0.0232, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2650, 0.0840, 0.0231, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2660, 0.0835, 0.0230, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2670, 0.0829, 0.0229, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2680, 0.0824, 0.0228, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2690, 0.0819, 0.0227, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2700, 0.0814, 0.0226, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2710, 0.0809, 0.0225, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2720, 0.0804, 0.0224, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2730, 0.0799, 0.0223, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2740, 0.0794, 0.0222, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2750, 0.0789, 0.0221, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2760, 0.0784, 0.0220, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2770, 0.0779, 0.0219, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2780, 0.0774, 0.0218, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2790, 0.0770, 0.0217, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2800, 0.0765, 0.0216, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2810, 0.0760, 0.0215, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2820, 0.0756, 0.0214, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2830, 0.0751, 0.0213, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2840, 0.0747, 0.0212, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2850, 0.0742, 0.0211, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2860, 0.0738, 0.0210, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2870, 0.0733, 0.0209, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2880, 0.0729, 0.0208, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2890, 0.0725, 0.0208, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2900, 0.0720, 0.0207, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2910, 0.0716, 0.0206, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2920, 0.0712, 0.0205, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2930, 0.0708, 0.0204, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2940, 0.0704, 0.0203, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2950, 0.0699, 0.0202, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2960, 0.0695, 0.0202, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2970, 0.0691, 0.0201, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2980, 0.0687, 0.0200, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 2990, 0.0683, 0.0199, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3000, 0.0679, 0.0198, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3010, 0.0675, 0.0197, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3020, 0.0672, 0.0197, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3030, 0.0668, 0.0196, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3040, 0.0664, 0.0195, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3050, 0.0660, 0.0194, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3060, 0.0656, 0.0193, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3070, 0.0653, 0.0193, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3080, 0.0649, 0.0192, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3090, 0.0645, 0.0191, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3100, 0.0642, 0.0190, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3110, 0.0638, 0.0190, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3120, 0.0634, 0.0189, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3130, 0.0631, 0.0188, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3140, 0.0627, 0.0187, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3150, 0.0624, 0.0187, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3160, 0.0620, 0.0186, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3170, 0.0617, 0.0185, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3180, 0.0614, 0.0184, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3190, 0.0610, 0.0184, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3200, 0.0607, 0.0183, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3210, 0.0604, 0.0182, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3220, 0.0600, 0.0181, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3230, 0.0597, 0.0181, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3240, 0.0594, 0.0180, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3250, 0.0590, 0.0179, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3260, 0.0587, 0.0179, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3270, 0.0584, 0.0178, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3280, 0.0581, 0.0177, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3290, 0.0578, 0.0177, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3300, 0.0575, 0.0176, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3310, 0.0572, 0.0175, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3320, 0.0569, 0.0175, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3330, 0.0565, 0.0174, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3340, 0.0562, 0.0173, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3350, 0.0559, 0.0173, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3360, 0.0556, 0.0172, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3370, 0.0554, 0.0171, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3380, 0.0551, 0.0171, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3390, 0.0548, 0.0170, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3400, 0.0545, 0.0169, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3410, 0.0542, 0.0169, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3420, 0.0539, 0.0168, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3430, 0.0536, 0.0168, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3440, 0.0533, 0.0167, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3450, 0.0531, 0.0166, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3460, 0.0528, 0.0166, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3470, 0.0525, 0.0165, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3480, 0.0522, 0.0165, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3490, 0.0520, 0.0164, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3500, 0.0517, 0.0163, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3510, 0.0514, 0.0163, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3520, 0.0512, 0.0162, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3530, 0.0509, 0.0162, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3540, 0.0507, 0.0161, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3550, 0.0504, 0.0161, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3560, 0.0501, 0.0160, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3570, 0.0499, 0.0159, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3580, 0.0496, 0.0159, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3590, 0.0494, 0.0158, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3600, 0.0491, 0.0158, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3610, 0.0489, 0.0157, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3620, 0.0486, 0.0157, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3630, 0.0484, 0.0156, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3640, 0.0482, 0.0155, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3650, 0.0479, 0.0155, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3660, 0.0477, 0.0154, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3670, 0.0474, 0.0154, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3680, 0.0472, 0.0153, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3690, 0.0470, 0.0153, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3700, 0.0467, 0.0152, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3710, 0.0465, 0.0152, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3720, 0.0463, 0.0151, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3730, 0.0460, 0.0151, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3740, 0.0458, 0.0150, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3750, 0.0456, 0.0150, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3760, 0.0454, 0.0149, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3770, 0.0451, 0.0149, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3780, 0.0449, 0.0148, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3790, 0.0447, 0.0148, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3800, 0.0445, 0.0147, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3810, 0.0443, 0.0147, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3820, 0.0441, 0.0146, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3830, 0.0438, 0.0146, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3840, 0.0436, 0.0145, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3850, 0.0434, 0.0145, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3860, 0.0432, 0.0144, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3870, 0.0430, 0.0144, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3880, 0.0428, 0.0143, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3890, 0.0426, 0.0143, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3900, 0.0424, 0.0142, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3910, 0.0422, 0.0142, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3920, 0.0420, 0.0141, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3930, 0.0418, 0.0141, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3940, 0.0416, 0.0141, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3950, 0.0414, 0.0140, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3960, 0.0412, 0.0140, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3970, 0.0410, 0.0139, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3980, 0.0408, 0.0139, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 3990, 0.0406, 0.0138, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4000, 0.0404, 0.0138, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4010, 0.0402, 0.0137, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4020, 0.0401, 0.0137, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4030, 0.0399, 0.0137, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4040, 0.0397, 0.0136, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4050, 0.0395, 0.0136, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4060, 0.0393, 0.0135, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4070, 0.0391, 0.0135, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4080, 0.0389, 0.0134, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4090, 0.0388, 0.0134, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4100, 0.0386, 0.0134, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4110, 0.0384, 0.0133, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4120, 0.0382, 0.0133, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4130, 0.0381, 0.0132, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4140, 0.0379, 0.0132, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4150, 0.0377, 0.0131, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4160, 0.0375, 0.0131, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4170, 0.0374, 0.0131, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4180, 0.0372, 0.0130, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4190, 0.0370, 0.0130, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4200, 0.0369, 0.0129, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4210, 0.0367, 0.0129, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4220, 0.0365, 0.0129, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4230, 0.0364, 0.0128, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4240, 0.0362, 0.0128, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4250, 0.0360, 0.0127, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4260, 0.0359, 0.0127, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4270, 0.0357, 0.0127, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4280, 0.0355, 0.0126, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4290, 0.0354, 0.0126, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4300, 0.0352, 0.0126, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4310, 0.0351, 0.0125, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4320, 0.0349, 0.0125, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4330, 0.0348, 0.0124, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4340, 0.0346, 0.0124, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4350, 0.0345, 0.0124, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4360, 0.0343, 0.0123, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4370, 0.0342, 0.0123, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4380, 0.0340, 0.0123, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4390, 0.0339, 0.0122, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4400, 0.0337, 0.0122, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4410, 0.0336, 0.0121, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4420, 0.0334, 0.0121, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4430, 0.0333, 0.0121, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4440, 0.0331, 0.0120, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4450, 0.0330, 0.0120, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4460, 0.0328, 0.0120, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4470, 0.0327, 0.0119, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4480, 0.0325, 0.0119, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4490, 0.0324, 0.0119, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4500, 0.0323, 0.0118, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4510, 0.0321, 0.0118, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4520, 0.0320, 0.0118, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4530, 0.0318, 0.0117, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4540, 0.0317, 0.0117, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4550, 0.0316, 0.0117, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4560, 0.0314, 0.0116, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4570, 0.0313, 0.0116, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4580, 0.0312, 0.0116, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4590, 0.0310, 0.0115, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4600, 0.0309, 0.0115, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4610, 0.0308, 0.0115, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4620, 0.0306, 0.0114, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4630, 0.0305, 0.0114, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4640, 0.0304, 0.0114, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4650, 0.0302, 0.0113, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4660, 0.0301, 0.0113, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4670, 0.0300, 0.0113, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4680, 0.0299, 0.0112, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4690, 0.0297, 0.0112, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4700, 0.0296, 0.0112, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4710, 0.0295, 0.0111, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4720, 0.0294, 0.0111, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4730, 0.0292, 0.0111, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4740, 0.0291, 0.0111, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4750, 0.0290, 0.0110, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4760, 0.0289, 0.0110, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4770, 0.0288, 0.0110, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4780, 0.0286, 0.0109, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4790, 0.0285, 0.0109, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4800, 0.0284, 0.0109, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4810, 0.0283, 0.0108, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4820, 0.0282, 0.0108, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4830, 0.0280, 0.0108, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4840, 0.0279, 0.0107, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4850, 0.0278, 0.0107, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4860, 0.0277, 0.0107, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4870, 0.0276, 0.0107, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4880, 0.0275, 0.0106, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4890, 0.0274, 0.0106, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4900, 0.0272, 0.0106, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4910, 0.0271, 0.0105, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4920, 0.0270, 0.0105, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4930, 0.0269, 0.0105, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4940, 0.0268, 0.0105, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4950, 0.0267, 0.0104, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4960, 0.0266, 0.0104, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4970, 0.0265, 0.0104, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4980, 0.0264, 0.0103, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 4990, 0.0263, 0.0103, 0.1000\n",
      "Iteration, Loss, Gradient Norm, Learning Rate: 5000, 0.0262, 0.0103, 0.1000\n"
     ]
    }
   ],
   "source": [
    "lr = .1\n",
    "prev_mloss = float('inf')  # Initialize with a large value\n",
    "no_improvement_count = 0\n",
    "lr_reduction_factor = 2.0\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    if i >5000: break\n",
    "    mloss, dldw_f = meta_loss(x=x_hat, y=y, model=model, dldw_target=dldw_target)\n",
    "    grad = torch.autograd.grad(mloss, x_hat)[0]\n",
    "\n",
    "    if i % 10 == 0 or i <10:\n",
    "        print('Iteration, Loss, Gradient Norm, Learning Rate: {:3d}, {:.4f}, {:.4f}, {:.4f}'.format(i, mloss.item(), grad.norm().item(), lr))\n",
    "\n",
    "    # Check if meta loss has not improved for the last 1000 iterations\n",
    "    if i > 1000 and mloss >= prev_mloss:\n",
    "        no_improvement_count += 1\n",
    "        if no_improvement_count >= 1000:\n",
    "            lr /= lr_reduction_factor  # Reduce learning rate by half\n",
    "            no_improvement_count = 0  # Reset no improvement counter\n",
    "\n",
    "    x_hat = x_hat - lr * grad\n",
    "    prev_mloss = mloss  # Update previous meta loss for the next iteration\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_rec = torch.sigmoid(x_hat) > .5\n",
    "plt.imshow(x_rec.detach().numpy().squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {labels[0].item()}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_reveal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
